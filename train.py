import argparse
import os
import tensorflow as tf
import time
from datetime import datetime
from trucks_and_drones.build_env import BuildEnvironment
import maddpg.trainer.utils as U
from maddpg.trainer.maddpg import MADDPGAgentTrainer
from tensorflow.keras import layers
from trucks_and_drones.config import cfg
import numpy as np

def parse_args():
    parser = argparse.ArgumentParser("Multi-Agent RL experiments")

    # ========== æ–°å¢:ç®—æ³•é€‰æ‹©å‚æ•° ==========
    parser.add_argument("--algorithm", type=str, default="maddpg",
                       choices=["maddpg", "iql", "ma2c", "coma", "cima","mappo"],
                       help="Algorithm to use: maddpg/iql/ma2c/coma/cima/mappo")

    # åœ¨parse_args()ä¸­æ·»åŠ MAPPOå‚æ•°
    parser.add_argument("--clip_param", type=float, default=0.2,
                        help="PPO clip parameter epsilon")
    parser.add_argument("--gae_lambda", type=float, default=0.95,
                        help="GAE lambda parameter")
    parser.add_argument("--value_coef", type=float, default=0.5,
                        help="Value loss coefficient")

    # Environment parameters
    parser.add_argument("--env-config", type=str, default=None,
                        help="Path to Python script that configures BuildEnvironment")
    parser.add_argument("--max-episode-len", type=int, default=1000,
                        help="maximum episode length")
    parser.add_argument("--num-episodes", type=int, default=60000,
                        help="number of episodes")
    # Core training parameters
    parser.add_argument("--lr_actor", type=float, default=1e-4,
                        help="learning rate for actor")
    parser.add_argument("--lr_critic", type=float, default=3e-4,
                        help="learning rate for critic")
    parser.add_argument("--gamma", type=float, default=0.95,
                        help="discount factor")
    parser.add_argument("--batch-size", type=int,
                        help="batch size")
    parser.add_argument("--num-units", type=int, default=64,
                        help="number of units in the MLP layers")
    parser.add_argument("--update-freq", type=int,
                        help="how often (in training steps) to update networks")

    # Checkpointing
    parser.add_argument("--exp-name", type=str, default="experiment",
                        help="name of the experiment (used for saving)")
    parser.add_argument("--save-dir", type=str, default="./checkpoints/",
                        help="directory in which to save models")
    parser.add_argument("--save-rate", type=int, default=2000,
                        help="save model once every this many episodes")

    # TensorBoard logging
    parser.add_argument("--log-dir", type=str, default="./logs/",
                        help="directory for TensorBoard logs")

    # Flags for PER and IAM
    parser.add_argument("--use-per", action="store_true", default=False,
                        help="use prioritized experience replay")
    parser.add_argument("--use-iam", action="store_true", default=False,
                        help="use invalid action masking")

    # æ·»åŠ å¯è§†åŒ–æ§åˆ¶å‚æ•°
    parser.add_argument("--visualize", action="store_true", default=False,
                        help="Enable visualization during training")
    parser.add_argument("--visualize-interval", type=int, default=1,
                        help="Visualize every N episodes (only when --visualize is enabled)")
    

    # PER parameters
    parser.add_argument("--per_mu", type=float, default=0.6,
                        help="Priority exponent mu for PER (Equation 33)")
    parser.add_argument("--per_sigma", type=float, default=1.0,
                        help="Sampling probability exponent sigma for PER (Equation 34)")
    parser.add_argument("--per_eta", type=float, default=1e-6,
                        help="Small constant eta to avoid zero priority")
    parser.add_argument("--per_beta", type=float, default=0.4,
                        help="Importance sampling exponent beta for PER")

    parser.add_argument("--hidden_units_actor", nargs='+', type=int, default=[512, 256],
                        help="Hidden layer sizes for actor network")
    parser.add_argument("--hidden_units_critic", nargs='+', type=int, default=[1024, 512, 256],
                        help="Hidden layer sizes for critic network")
    parser.add_argument("--activation", type=str, default="relu", help="Activation function: relu or tanh")
    parser.add_argument("--epsilon", type=float, default=0.1,
                        help="Epsilon value for epsilon-greedy exploration")

    return parser.parse_args()


def mlp_actor(input, num_outputs, scope, reuse=False):
    x = layers.Dense(512, activation='relu')(input)
    x = layers.Dense(256, activation='relu')(x)
    out = layers.Dense(num_outputs)(x)
    return out


def mlp_critic(input, num_outputs, scope, reuse=False):
    x = layers.Dense(1024, activation='relu')(input)
    x = layers.Dense(512, activation='relu')(x)
    x = layers.Dense(256, activation='relu')(x)
    out = layers.Dense(num_outputs)(x)
    return out

# çº¿æ€§ Îµ è°ƒåº¦ï¼šwarmup æ­¥å†…ä¸º 1.0ï¼Œä¹‹ååœ¨ decay_steps å†…ä¸‹é™åˆ° 0.1
def get_epsilon(step, warmup_steps=1500, eps_max=1.0, eps_min=0.1, decay_steps=30000):
    if step < warmup_steps:
        return eps_max
    # ratio = min(1.0, max(0.0, (step - warmup_steps) / float(decay_steps)))
    # return eps_max + (eps_min - eps_max) * ratio
    # â­ ä½¿ç”¨æŒ‡æ•°è¡°å‡æ›¿ä»£çº¿æ€§è¡°å‡
    decay_ratio = (step - warmup_steps) / float(decay_steps)
    decay_ratio = min(1.0, max(0.0, decay_ratio))
    return eps_min + (eps_max - eps_min) * np.exp(-3 * decay_ratio)  # æŒ‡æ•°è¡°å‡

def train(args):
    # ========== åŠ¨æ€å¯¼å…¥ç®—æ³• ==========
    if args.algorithm == "maddpg":
        from maddpg.trainer.maddpg import MADDPGAgentTrainer as TrainerClass
    elif args.algorithm == "ma2c":
        from maddpg.trainer.ma2c import MA2CAgentTrainer as TrainerClass
    elif args.algorithm == "mappo":
        from maddpg.trainer.mappo import MAPPOAgentTrainer as TrainerClass
    elif args.algorithm == "coma":
        from maddpg.trainer.coma import COMAAgentTrainer as TrainerClass
    elif args.algorithm == "cima":
        from maddpg.trainer.cima import CIMAAgentTrainer as TrainerClass

    print(f"ğŸ¤– Using algorithm: {args.algorithm.upper()}")
    
    # è®¾ç½®TensorBoardæ—¥å¿—è®°å½•
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    log_dir = os.path.join(args.log_dir, f"{args.exp_name}_{timestamp}")
    os.makedirs(log_dir, exist_ok=True)
    
    # åˆ›å»ºTensorBoard writers
    train_writer = tf.summary.create_file_writer(os.path.join(log_dir, 'train'))
    episode_writer = tf.summary.create_file_writer(os.path.join(log_dir, 'episode'))
    
    print(f"ğŸ“Š TensorBoard logs will be saved to: {log_dir}")
    print(f"ğŸ“ˆ Run 'tensorboard --logdir {log_dir}' to view training curves")
    
    # è¦†ç›–å‘½ä»¤è¡Œå‚æ•°ï¼šè¯»å– config ä¸­çš„è®­ç»ƒè½®æ•°
    args.num_episodes = cfg['training']['num_episodes']
    args.batch_size   = cfg['training']['batch_size']
    args.update_freq  = cfg['training']['update_freq']
    args.target_update_freq = cfg['training']['target_update_freq']
    args.min_buffer_size = cfg['training']['min_buffer_size']
    args.updates_per_call   = cfg['training'].get('updates_per_call')
    args.warmup_steps       = cfg['training'].get('warmup_steps')
    args.entropy_coef       = cfg['training'].get('entropy_coef')
    args.tau                = cfg['training'].get('tau')
    args.lr_actor   = cfg['training'].get('lr_actor')
    args.lr_critic  = cfg['training'].get('lr_critic')
    args.rollout_len = cfg['training'].get('rollout_len')

    # 1) ä» cfg é‡Œè¯»å–ç¯å¢ƒé…ç½®
    env_cfg = cfg.get('environment', {})
    num_trucks    = env_cfg.get('num_trucks')
    num_drones    = env_cfg.get('num_drones')
    num_depots    = env_cfg.get('num_depots')
    num_customers = env_cfg.get('num_customers')


    # 2) ç¡®å®šè§‚æµ‹å­—æ®µï¼šç”¨ temp_db é‡Œå®é™…æœ‰çš„ 'n_items' è€Œä¸æ˜¯ 'demand'
    obs_contin   = [ 'TW', 'time', 'n_items', 'deadline']
    obs_discrete = ['ET', 'ED', 'NT', 'ND', 'truck_node', 'drone_node', 'delta']
    # 'delta'æ˜¯â€œunassignedâ€çš„åº•å±‚å­—æ®µ
    tandem_fields = obs_discrete + obs_contin + ['LT_time', 'LD_time']


    # â€”â€” 3) ä¸€æ¬¡æ€§æ„é€ ç¯å¢ƒ â€”â€”
    builder = (
        BuildEnvironment(
            "VRPD",
            enable_visualization=args.visualize  # ä¼ é€’å¯è§†åŒ–å¼€å…³
        )
        .trucks(num=num_trucks)
        .drones(num=num_drones)
        .depots(num=num_depots)
        .customers(num=num_customers)
        .visuals()
        .observations(
            contin_inputs=obs_contin,
            discrete_inputs=obs_discrete,  # ç”¨ NT/ND
            combine_per_index=[tandem_fields for _ in range(num_trucks)],
            combine_per_type=['vehicles', 'customers', 'depots'],  # å…³é”®ï¼šå…¨å±€è§‚æµ‹æ¥æº
            flatten=True,  # æŠŠæ¯ä¸ª agent çš„è§‚æµ‹æ‰å¹³åŒ–
            output_as_array=False, # è¿”å› list_of_agent_obs è€Œä¸æ˜¯å¤§å‘é‡
        )
        .actions()
        .rewards()
        .compile()

    )
    env = builder.build()
    # env.visualizer.visualize_step(episode=0, step=0)

#     from trucks_and_drones.simulation.temp_database import BaseTempDatabase  # åªæ˜¯ç±»å‹æç¤ºï¼Œä¸å¿…ä¸€å®šå¯¼
#     from trucks_and_drones.simulation.temp_database import export_instance_coords  # 

#     # å¯¼å‡ºâ€œé™æ€åŸºçº¿ç”¨â€çš„åæ ‡ï¼ˆåªå« depot+é™æ€å®¢æˆ·ï¼‰
#     export_instance_coords(env.temp_db, path="rl_instance_coords.json", include_dynamic=False)
#     print("âœ… RL å®ä¾‹åæ ‡å·²å¯¼å‡ºåˆ° rl_instance_coords.json")

    # 4) å¤šæ™ºèƒ½ä½“è®­ç»ƒå‡†å¤‡
    n_agents = len(builder.trucks) + len(builder.drones)
    
    # obs_shape_n = [env.observation_space[i].shape for i in range(n_agents)]
    obs_n, global_obs = env.reset()
    obs_shape_n = [obs.shape for obs in obs_n]
    
    # è·å–å…¨å±€è§‚æµ‹ç»´åº¦
    global_obs_dim = global_obs.shape[0] if hasattr(global_obs, 'shape') else len(global_obs)
    print(f"ğŸŒ Global observation dimension: {global_obs_dim}")
    print(">>> obs_shape_n =", obs_shape_n)
    # è¯¦ç»†åˆ†æè§‚æµ‹ç©ºé—´ç»„æˆ
    print(f"\nğŸ”¬ === è§‚æµ‹ç©ºé—´ç»„æˆåˆ†æ ===")
    
    # ä»temp_dbè·å–è¯¦ç»†ä¿¡æ¯è¿›è¡Œè°ƒè¯•
    temp_db = builder.temp_db
    print(f"å¡è½¦çŠ¶æ€ (ET): {temp_db.status_dict['ET']}")
    print(f"æ— äººæœºçŠ¶æ€ (ED): {temp_db.status_dict['ED']}")
    print(f"å¡è½¦è½½é‡ (TW): {temp_db.status_dict['TW']}")
    print(f"æ— äººæœºè½½é‡ (DW): {temp_db.status_dict['DW']}")
    print(f"èŠ‚ç‚¹éœ€æ±‚ (n_items): {temp_db.status_dict['n_items']}")
    print(f"èŠ‚ç‚¹æˆªæ­¢æ—¶é—´ (deadline): {temp_db.constants_dict.get('deadline', 'Not found')}")
    print(f"æœªåˆ†é…çŠ¶æ€ (delta): {temp_db.status_dict['delta']}")


    act_space_n = list(env.action_space.spaces)
    
    # print("Observation space:", env.observation_space)
    print("Type:", type(env.observation_space))
    # for i, space in enumerate(env.observation_space):
    #     print(f" - Obs {i}: shape = {space.shape}, low = {space.low}, high = {space.high}")

    raw_aspace = env.act_decoder.action_space()
    # åŸæ¥ raw_aspace æ˜¯ Tuple(Tuple(Discrete,â€¦), â€¦)ï¼Œå…ˆ unpackæˆ act_space_listï¼š
    act_space_list = raw_aspace.spaces[0].spaces  # å–ç¬¬ä¸€ä¸ª agent çš„ â€œtuple of Discreteâ€    

    trainers = []
    for i in range(n_agents):
        trainers.append(
            TrainerClass(
                name=f"agent_{i}",
                obs_shape_n=obs_shape_n,
                act_space_list=act_space_list,
                agent_index=i,
                args=args,
                global_obs_dim=global_obs_dim,
                # æ·»åŠ ç¼ºå¤±çš„ç¯å¢ƒå‚æ•°
                num_trucks=num_trucks,
                num_drones=num_drones,
                total_nodes=num_depots + num_customers
            )
        )

    # 5) åˆ›å»ºä¿å­˜ç›®å½•å¹¶ä¿å­˜åˆå§‹æ¨¡å‹
    save_path = args.save_dir
    os.makedirs(save_path, exist_ok=True)
    train_step = 0
    
    # 6) è®­ç»ƒå¾ªç¯
    episode_rewards = [0.0]
    episode_step = 0
    obs_n, global_obs = env.reset()
    
    # æ–°å¢ï¼šä¸“ç”¨â€œæ›´æ–°è®¡æ•°å™¨â€ï¼Œè®©lossæ›²çº¿è¿ç»­
    update_steps = 0

    # ========== æ–°å¢ï¼šåˆå§‹åŒ–æ€»æ­¥æ•°è®¡æ•°å™¨ ==========
    env.visualizer.total_training_steps = 0
    # ==========================================

    # åˆå§‹åŒ–å¯è§†åŒ– - æ˜¾ç¤ºåˆå§‹çŠ¶æ€
    if args.visualize:
        print("ğŸ¬ Starting visualization...")
        env.visualizer.visualize_step(
            episode=len(episode_rewards)-1,
            step=episode_step,
            slow_down_pls=True,
            last_actions=None,
            last_rewards=None
        )
    print(f"Starting training with PER={args.use_per}, IAM={args.use_iam}")
    # å­˜å‚¨æœ€åçš„åŠ¨ä½œç”¨äºæ˜¾ç¤º
    last_actions = []
    last_rewards = []
    
    # ç”¨äºè®°å½•è®­ç»ƒæŒ‡æ ‡
    total_critic_loss = 0.0
    total_actor_loss = 0.0
    num_updates = 0

    while True:
        # # ç”ŸæˆåŠ¨ä½œæ©ç 
        # mask_n = [env.get_mask() for _ in range(n_agents)] if args.use_iam else [None] * n_agents

        mask_n = [env.get_mask(agent_index=i) for i in range(n_agents)] if args.use_iam else [None] * n_agents

        # === Îµ æ¢ç´¢ï¼ˆä½¿ç”¨ warmup_stepsï¼‰===
        ws = args.warmup_steps if args.warmup_steps is not None else 0
        cur_eps = get_epsilon(train_step, warmup_steps=ws, eps_max=1.0, eps_min=0.1, decay_steps=30000)
        # for tr in trainers:
        #     tr.actor.epsilon = cur_eps
        for tr in trainers:
            # âœ… åªæœ‰MADDPGæœ‰epsilonå±æ€§ï¼ŒMA2Cæ²¡æœ‰å°±è·³è¿‡
            if hasattr(tr, 'actor') and hasattr(tr.actor, 'epsilon'):
                tr.actor.epsilon = cur_eps

        # æ™ºèƒ½ä½“å†³ç­–
        if args.algorithm.lower() in ['ma2c', 'mappo']:
            # MA2C çš„ action(obs, global_obs, mask)
            action_n = [tr.action(o, global_obs, m) for tr, o, m in zip(trainers, obs_n, mask_n)]
        else:
            action_n = [tr.action(o, m) for tr, o, m in zip(trainers, obs_n, mask_n)]
            last_actions = action_n.copy()  # ä¿å­˜ç”¨äºæ˜¾ç¤º

        print(f"\nğŸ¯ Episode {len(episode_rewards)-1}, Step {episode_step}")
        print(f"   Generated actions: {action_n}")


        # (new_obs_n, new_global_obs), rew_n, done, _ = env.step(action_n)
        (new_obs_n, new_global_obs), rew_n, done, info = env.step(action_n)

        # ä¼˜å…ˆç”¨ info é‡Œçš„è¢«æ‰§è¡ŒåŠ¨ä½œï¼›è‹¥ä¸Šå±‚æ²¡é€ä¼ ï¼Œåˆ™å›é€€åˆ° temp_db ç¼“å­˜ï¼›å†ä¸è¡Œæ‰é€€å›åŸåŠ¨ä½œ
        executed_action_n = None
        if isinstance(info, dict):
            executed_action_n = info.get('executed_actions_multihead', None)
        if executed_action_n is None:
            executed_action_n = getattr(env.temp_db, 'last_executed_actions_multihead', None)
        if executed_action_n is None:
            # æœ€åçš„å…œåº•ï¼ˆä¸å»ºè®®é•¿æœŸä½¿ç”¨ï¼‰
            executed_action_n = action_n

        last_actions = executed_action_n.copy()  # ä½ åé¢æœ‰å¯è§†åŒ–ï¼Œè¿™é‡Œä¹ŸåŒæ­¥æˆâ€œè¢«æ‰§è¡ŒåŠ¨ä½œâ€

        # ä¾¿äºäººçœ‹ï¼šæŠŠ list[list[int]] å‹æˆ tuple
        def _summ(a): 
            try: return [tuple(x) for x in a]
            except: return a

        print(f"   Proposed actions: {_summ(action_n)}")
        print(f"   â–¶ï¸ Executed actions (used for replay): {_summ(executed_action_n)}")

        # å·®å¼‚ç»Ÿè®¡ï¼ˆæ¯ä¸ª agent çš„å¤šå¤´æ˜¯å¦ä¸€è‡´ï¼‰
        try:
            diffs = [int(tuple(pa) != tuple(ea)) for pa, ea in zip(action_n, executed_action_n)]
            print(f"   Diff agents (proposedâ‰ executed): {sum(diffs)}/{len(diffs)}")
        except Exception as e:
            print(f"   [Warn] diff check failed: {e}")

        last_rewards = rew_n.copy()  # ä¿å­˜ç”¨äºæ˜¾ç¤º

        # === å…³é”®ï¼šæŠŠ next_obsï¼ˆæ¯ä¸ª agent ä¸€æ¡ 1D å‘é‡ï¼‰å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦ ===
        def pad1d_list(xs, fill=0.0):
            arrs = [np.asarray(x, dtype=np.float32).reshape(-1) for x in xs]
            maxlen = max(a.shape[0] for a in arrs)
            out = [np.pad(a, (0, maxlen - a.shape[0]), mode="constant", constant_values=fill) for a in arrs]
            return np.stack(out, axis=0), maxlen  # (n_agents, maxlen)

        next_obs_all_pad, _ = pad1d_list(new_obs_n)
        next_mask_n = [env.get_mask(agent_index=i) for i in range(n_agents)] if args.use_iam else [None]*n_agents

        # # **å…³é”®ï¼šç”¨æ–°çŠ¶æ€å†å–ä¸‹ä¸€æ­¥çš„ mask**
        # next_mask_n = [env.get_mask(agent_index=i) for i in range(n_agents)] if args.use_iam else [None] * n_agents


        # âœ… experienceè°ƒç”¨å®Œå…¨ç›¸åŒï¼ŒMADDPGå’ŒMA2Céƒ½ç”¨è¿™ä¸ªæ¥å£
        for i, tr in enumerate(trainers):
            try:
                print(f"   [Replay] About to store act_all sample (agent0 view): {executed_action_n[0]}")
                print(f"   [Replay] act_all len = {len(executed_action_n)}")
                # ç®€å•å½¢çŠ¶æ£€æŸ¥
                assert len(executed_action_n) == n_agents, \
                    f"act_all length {len(executed_action_n)} != n_agents {n_agents}"
            except Exception as e:
                print(f"   [Replay] Pre-store check warn: {e}")

            tr.experience(
                obs=obs_n[i],                       # æœ¬æ™ºèƒ½ä½“å½“å‰è§‚æµ‹
                global_obs=global_obs,              # å…¨å±€è§‚æµ‹
                # act_all=action_n,                   # æ‰€æœ‰æ™ºèƒ½ä½“æœ¬æ­¥åŠ¨ä½œï¼ˆç¦»æ•£å¤šå¤´ï¼‰
                act_all=executed_action_n,
                rew=rew_n[i],                       # åä½œè®¾å®šä¸‹ç›¸åŒï¼Œä¿ç•™ä¸ºæ ‡é‡
                next_obs=new_obs_n[i],              # æœ¬æ™ºèƒ½ä½“ä¸‹ä¸€è§‚æµ‹ï¼ˆç»™ actor ç”¨ï¼‰
                next_obs_all=next_obs_all_pad,          # æ‰€æœ‰æ™ºèƒ½ä½“ä¸‹ä¸€è§‚æµ‹ï¼ˆç»™ target Q ç”¨ï¼‰
                next_global_obs=new_global_obs,
                done=done,
                mask_all=mask_n,
                next_mask_all=next_mask_n
            )
            # tr.experience(
            #     obs_n[i], global_obs, action_n[i], rew_n[i],
            #     new_obs_n[i], new_global_obs, done
            # )

        obs_n = new_obs_n
        global_obs = new_global_obs
        episode_step += 1
        train_step += 1       

        episode_rewards[-1] += rew_n[0]  # å› ä¸ºæ‰€æœ‰æ™ºèƒ½ä½“çš„å¥–åŠ±ç›¸åŒï¼Œåªå–ç¬¬ä¸€ä¸ª
        
        # å¯è§†åŒ–å½“å‰æ­¥éª¤ - æ ¹æ®å‚æ•°å’Œé—´éš”å†³å®š
        if args.visualize and len(episode_rewards) % args.visualize_interval == 0:
            env.visualizer.visualize_step(
                episode=len(episode_rewards)-1,
                step=episode_step,
                slow_down_pls=(episode_step < 10),
                last_actions=last_actions,
                last_rewards=last_rewards
            )

        # æ¯5æ­¥æš‚åœè®©ç”¨æˆ·è§‚å¯Ÿ
        if episode_step % 5 == 0 and episode_step > 0:
            print(f"\nğŸ“Š Episode {len(episode_rewards)-1}, Step {episode_step} Summary:")
            print(f"   Total reward so far: {episode_rewards[-1]:.3f}")
            print(f"   Step rewards: {rew_n}")
            print(f"   Actions taken: {last_actions}")
            print(f"   Vehicle positions: {env.temp_db.status_dict.get('v_coord', 'N/A')}")
            print(f"   Vehicle status ET: {env.temp_db.status_dict.get('ET', 'N/A')}")
            print(f"   Vehicle status ED: {env.temp_db.status_dict.get('ED', 'N/A')}")

            # æ£€æŸ¥æ˜¯å¦æœ‰èŠ‚ç‚¹è¢«è®¿é—®
            visited = getattr(env.temp_db, 'visited_nodes', set())
            delta = env.temp_db.get_val('delta') if hasattr(env.temp_db, 'get_val') else None
            print(f"   Visited nodes: {visited}")
            print(f"   Delta status: {delta}")

            # if episode_step < 2000:  # å‰20æ­¥æ‰‹åŠ¨æ§åˆ¶
            #     input("   Press Enter to continue...")
        
        # è®­ç»ƒæ™ºèƒ½ä½“ - ä¿®æ”¹è¿™éƒ¨åˆ†
        # ===== å‚æ•°æ›´æ–°ï¼ˆé¢„çƒ­æœŸå†…è·³è¿‡ï¼‰=====
        if args.algorithm.lower() in ["ma2c", "mappo"]:
            # On-policyï¼šæ¯æ­¥éƒ½å°è¯•æ›´æ–°ï¼Œupdate() å†…éƒ¨ä¼šåœ¨å›åˆç»“æŸæ—¶æ‰çœŸæ­£ä¼˜åŒ–
            for tr in trainers:
                losses = tr.update(trainers, train_step)
                if losses is not None and len(losses) == 2:
                    critic_loss, actor_loss = losses
                    with train_writer.as_default():
                        tf.summary.scalar(f'Agent_{tr.agent_index}/Critic_Loss', float(critic_loss), step=train_step)
                        tf.summary.scalar(f'Agent_{tr.agent_index}/Actor_Loss', float(actor_loss), step=train_step)
                        tf.summary.scalar(f'Agent_{tr.agent_index}/Abs_Actor_Loss', abs(float(actor_loss)), step=train_step)
            train_writer.flush()
        else:
            # Off-policyï¼ˆå¦‚ MADDPGï¼‰ä¿æŒåŸæ¥çš„æœ€å°ç¼“å†²/æ›´æ–°é¢‘ç‡åˆ¤æ–­
            if (len(trainers[0].buffer) >= args.min_buffer_size and
                train_step % args.update_freq == 0 and
                train_step >= (args.warmup_steps or 0)):  # â† é¢„çƒ­æœªç»“æŸæ—¶ä¸æ›´æ–°

                for tr in trainers: 
                    tr.preupdate()
                
                # â­ å¤šæ¬¡æ¢¯åº¦æ­¥ï¼šé»˜è®¤ 1 æ¬¡ï¼›é…ç½®ä¸º N å°±åš N æ¬¡
                updates = int(args.updates_per_call or 1)
                for _ in range(updates):
                    for tr in trainers:
                        losses = tr.update(trainers, train_step, target_update_freq=args.target_update_freq)
                        if losses is None or len(losses) != 2:
                            # æœ¬æ¬¡æ²¡æ›´æ–°ï¼ˆä¾‹å¦‚ç¼“å†²ä¸å¤Ÿæˆ–lossä¸ºNaNè¢«è·³è¿‡ï¼‰ï¼Œç›´æ¥ä¸‹ä¸€ä¸ª
                            continue

                        critic_loss, actor_loss = losses
                        with train_writer.as_default():
                            tf.summary.scalar(f'Agent_{tr.agent_index}/Critic_Loss', float(critic_loss), step=train_step)
                            tf.summary.scalar(f'Agent_{tr.agent_index}/Actor_Loss',  float(actor_loss),  step=train_step)
                            tf.summary.scalar(f'Agent_{tr.agent_index}/Abs_Actor_Loss', abs(float(actor_loss)), step=train_step)

                        print(f"   ğŸ“Š Agent {tr.agent_index} - Critic Loss: {critic_loss:.6f}, Actor Loss: {actor_loss:.6f}")

                    train_writer.flush()
                     
                
            else:
                print(f"   Buffer size {len(trainers[0].buffer)} < min required {args.min_buffer_size}, skipping training")


        if done:
            print(f"\nğŸ Episode {len(episode_rewards)-1} completed!")
            print(f"   Final reward: {episode_rewards[-1]:.2f}")
            print(f"   Episode length: {episode_step}")
            print(f"   Reason: {'Task completed' if done else 'Max steps reached'}")

            # è·å–æˆæœ¬ç»Ÿè®¡
            cost_stats = env.reward_calc.get_episode_statistics()
            print(f"\nğŸ“Š Episode Cost Statistics:")
            print(f"   Total Cost (TC): {cost_stats['total_cost']:.2f}")
            print(f"   Travel Cost (Tra-C): {cost_stats['travel_cost']:.2f}")
            print(f"   Delay Penalty: {cost_stats['delay_penalty']:.2f}")
            print(f"   Unserved Penalty: {cost_stats['unserved_penalty']:.2f}")
            print(f"   Unserved Nodes: {cost_stats['unserved_count']}")
            print(f"   Served Nodes: {cost_stats['served_count']}")
            print(f"   Truck Travel Time: {cost_stats['truck_travel_time']:.2f}")
            print(f"   Drone Travel Time: {cost_stats['drone_travel_time']:.2f}")

            # è®°å½•episodeç»Ÿè®¡åˆ°TensorBoard
            episode_reward = episode_rewards[-1]
            episode_num = len(episode_rewards)-1
            
            with episode_writer.as_default():
                # è®°å½•episodeæ€»å¥–åŠ±
                tf.summary.scalar('Episode/Total_Reward', episode_reward, step=episode_num)
                tf.summary.scalar('Episode/Episode_Length', episode_step, step=episode_num)
                # è®°å½•æˆæœ¬ç»Ÿè®¡
                tf.summary.scalar('Costs/Total_Cost', cost_stats['total_cost'], step=episode_num)
                tf.summary.scalar('Costs/Travel_Cost', cost_stats['travel_cost'], step=episode_num)
                tf.summary.scalar('Costs/Delay_Penalty', cost_stats['delay_penalty'], step=episode_num)
                tf.summary.scalar('Costs/Unserved_Penalty', cost_stats['unserved_penalty'], step=episode_num)
                tf.summary.scalar('Costs/Unserved_Count', cost_stats['unserved_count'], step=episode_num)
                tf.summary.scalar('Costs/Served_Count', cost_stats['served_count'], step=episode_num)
                tf.summary.scalar('Time/Truck_Travel_Time', cost_stats['truck_travel_time'], step=episode_num)
                tf.summary.scalar('Time/Drone_Travel_Time', cost_stats['drone_travel_time'], step=episode_num)
                
                # è®°å½•ç¯å¢ƒç»Ÿè®¡
                try:
                    visited_nodes = len(getattr(env.temp_db, 'visited_nodes', set()))
                    delta = env.temp_db.get_val('delta')
                    total_nodes = len(delta)
                    completion_rate = visited_nodes / max(total_nodes - 1, 1)  # æ’é™¤depot
                    
                    tf.summary.scalar('Episode/Completion_Rate', completion_rate, step=episode_num)
                    tf.summary.scalar('Episode/Visited_Nodes', visited_nodes, step=episode_num)
                    # tf.summary.scalar('Episode/Remaining_Nodes', total_nodes - visited_nodes - 1, step=episode_num)
                    remaining = int(np.sum(env.temp_db.get_val('delta')[1:] == 1))
                    tf.summary.scalar('Episode/Remaining_Nodes', remaining, step=episode_num)
                except Exception as e:
                    print(f"   âš ï¸  Could not record environment stats: {e}")
            
            # å¼ºåˆ¶å†™å…¥TensorBoard
            train_writer.flush()
            episode_writer.flush()

            # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€ - æ ¹æ®å‚æ•°å†³å®š
            if args.visualize and len(episode_rewards) % args.visualize_interval == 0:
                env.visualizer.visualize_step(
                    episode=len(episode_rewards)-1,
                    step=episode_step,
                    slow_down_pls=True,
                    last_actions=last_actions,
                    last_rewards=last_rewards
                )

            # input("   Episode finished. Press Enter to start next episode...")
            obs_n, global_obs = env.reset()
            print(f"Episode {len(episode_rewards)-1} reward: {episode_rewards[-1]:.2f}")


            # é‡ç½®episodeç»Ÿè®¡
            episode_step = 0
            episode_rewards.append(0.0)
            total_critic_loss = 0.0
            total_actor_loss = 0.0
            num_updates = 0

            # æ¯400ä¸ªepisodeä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if len(episode_rewards) % args.save_rate == 0 and len(episode_rewards) > 1:
                for i, agent in enumerate(trainers):
                    if args.algorithm.lower() == "ma2c":
                        # MA2Cï¼šä¸€ä¸ª policy åŒæ—¶åŒ…å« actor+critic
                        agent.policy.save_weights(f"{save_path}/agent_{i}_policy_ep{len(episode_rewards)}.h5")
                    elif args.algorithm.lower() == "mappo":
                        # MAPPOï¼šä¿å­˜æ•´ä¸ª policyï¼ˆå« actor heads + criticï¼‰
                        agent.policy.save_weights(f"{save_path}/agent_{i}_mappo_policy_ep{len(episode_rewards)}.weights.h5")
                    else:
                        agent.actor.model.save_weights(f"{save_path}/agent_{i}_actor_weights_ep{len(episode_rewards)}.h5")
                        agent.critic.model.save_weights(f"{save_path}/agent_{i}_critic_weights_ep{len(episode_rewards)}.h5")
                        agent.actor.target_model.save_weights(f"{save_path}/agent_{i}_actor_target_weights_ep{len(episode_rewards)}.h5")
                        agent.critic.target_model.save_weights(f"{save_path}/agent_{i}_critic_target_weights_ep{len(episode_rewards)}.h5")
                print(f"Models saved at episode {len(episode_rewards)}")

            # æ˜¾ç¤ºæ–°episodeçš„åˆå§‹çŠ¶æ€
            env.visualizer.visualize_step(
                episode=len(episode_rewards)-1,
                step=episode_step,
                slow_down_pls=True,
                last_actions=None,
                last_rewards=None
            )


        # è¾¾åˆ°æœ€å¤§è®­ç»ƒè½®æ•°ï¼Œä¿å­˜æ¨¡å‹å¹¶é€€å‡º
        if len(episode_rewards) > args.num_episodes:
            print(f"\nğŸ“ Training completed after {args.num_episodes} episodes!")
            for i, agent in enumerate(trainers):
                if args.algorithm.lower() == "ma2c":
                    # MA2Cï¼šåªä¿å­˜ policy
                    agent.policy.save_weights(f"{save_path}/agent_{i}_ma2c_policy_final.h5")
                elif args.algorithm.lower() == "mappo":
                    agent.policy.save_weights(f"{save_path}/agent_{i}_mappo_policy_final.weights.h5")
                else:
                    # MADDPGï¼šä¿å­˜ actor/critic åŠå…¶ target
                    agent.actor.model.save_weights(f"{save_path}/agent_{i}_actor_final_weights.h5")
                    agent.critic.model.save_weights(f"{save_path}/agent_{i}_critic_final_weights.h5")
                    agent.actor.target_model.save_weights(f"{save_path}/agent_{i}_actor_target_final_weights.h5")
                    agent.critic.target_model.save_weights(f"{save_path}/agent_{i}_critic_target_final_weights.h5")
            print("Final model weights saved.")

            # å…³é—­TensorBoard writers
            train_writer.close()
            episode_writer.close()
            print(f"ğŸ“Š TensorBoard logs saved to: {log_dir}")
            break

if __name__ == '__main__':
    args = parse_args()
    train(args)
